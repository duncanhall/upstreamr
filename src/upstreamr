#!/usr/bin/env python
# Copyright (c) Trainline Limited. All rights reserved. See LICENSE.txt in the project root for license information.
# vim: tabstop=4 expandtab shiftwidth=4 softtabstop=4

import os
import sys
import fcntl
import socket
import argparse
import ConfigParser
import subprocess
import time
import calendar
import random
import logging
import re
import multiprocessing
import multiprocessing_logging
import traceback
import StringIO
import dns.resolver
import six
import simplejson
import requests
from jinja2 import Environment, FileSystemLoader
import boto.dynamodb2
import boto.sns
from boto.dynamodb2.table import Table

def install_mp_handler(logger=None):
    """Wraps the handlers in the given Logger with an MultiProcessingHandler.
    :param logger: whose handlers to wrap. By default, the root logger.
    """
    if logger is None:
        logger = logging.getLogger()

    for i, orig_handler in enumerate(list(logger.handlers)):
        handler = MultiProcessingHandler(
            'mp-handler-{0}'.format(i), sub_handler=orig_handler)

        logger.removeHandler(orig_handler)
        logger.addHandler(handler)

def process_name():
    mp_name = multiprocessing.current_process().name
    if mp_name is None:
        mp_name = "Main"
    return mp_name

def function_name():
    """ Returns the name of the function calling this code """
    return traceback.extract_stack(None, 3)[0][2]

class log_wrapper:
    """ Instanciates logging wrapper to add useful information to all logs without repeating code """
    def __init__(self):
        # Initialise logger
        self.logger = logging.getLogger('upstreamr')
        # Multiprocessing safe logging module
        install_mp_handler()
    def debug(self, message):
        self.logger.debug("%s %s - %s", process_name(), function_name(), message)
    def info(self, message):
        self.logger.info("%s %s - %s", process_name(), function_name(), message)
    def warn(self, message):
        self.logger.warn("%s %s - %s", process_name(), function_name(), message)
    def error(self, message):
        self.logger.error("%s %s - %s", process_name(), function_name(), message, exc_info=True)
    def critical(self, message):
        self.logger.critical("%s %s - %s", process_name(), function_name(), message, exc_info=True)

def aws_authentication(args):
    """ Performs AWS authentication, preferred choice is IAM Role """
    log = log_wrapper()
    log.info('Requesting AWS credentials')
    # Connect to AWS
    # Connect through the inherited IAM role if we have one
    aws_access_key_id = False
    aws_secret_access_key = False
    # If role is not available select them from vars or command line
    if args.profile:
        sts_connection = boto.connect_sts()
        instance_profile_arn = boto.utils.get_instance_metadata()['iam']['info']['InstanceProfileArn'].replace('instance-profile', 'role')
        try:
            assumed_role_object = sts_connection.assume_role(
                instance_profile_arn,
                role_session_name="Upstreamr"
            )
        except Exception as e:
            log.critical('Can\'t assume role through STS, exiting: %s' % e.message)
            sys.exit(1)
        log.info('AWS Authenticated through instance profile %s' % instance_profile_arn)
        aws_access_key_id = assumed_role_object.credentials.access_key
        aws_secret_access_key = assumed_role_object.credentials.secret_key
        aws_security_token = assumed_role_object.credentials.session_token
        aws_expiration_token = calendar.timegm(time.strptime(assumed_role_object.credentials.expiration, "%Y-%m-%dT%H:%M:%SZ"))
        log.debug('Credential Expiration in: %s' % assumed_role_object.credentials.expiration)
    else:
        log.info('AWS Authenticated using command line arguments')
        aws_access_key_id = args.aws_access_key
        aws_secret_access_key = args.aws_secret_key
        aws_security_token = None
        aws_expiration_token = 0.0

    # Check I've got credentials
    if aws_access_key_id and aws_secret_access_key:
        pass
    else:
        log.critical('We couldnt find any AWS credentials, exiting...')
        sys.exit(1)
    return aws_access_key_id, aws_secret_access_key, aws_security_token, aws_expiration_token

def dynamoconn():
    """ This gets a dynamodb connection """
    log = log_wrapper()
    # Define globals
    global _aws_expiration_token
    global _dynamoconn
    # Check existance of dynamoconn
    create_creds = False
    try:
        _dynamoconn.host
    except NameError:
        log.info('No DynamoDB connection found, creating new one')
        create_creds = True
    # Check the token validity
    try:
        _aws_expiration_token
    except NameError:
        create_creds = True
    if not create_creds:
        if _aws_expiration_token != 0.0 and _aws_expiration_token - time.time() < 300.0:
            log.info('DynamoDB connection expired, requesting new credentials')
            create_creds = True

    # If either is incorrect request new credentials and generate dynamoconn
    if create_creds:
        aws_access_key_id, aws_secret_access_key, aws_security_token, _aws_expiration_token = aws_authentication(args)
        log.debug('Creating DynamoDB connection')
        connect_try = True
        while connect_try:
            try:
                _dynamoconn = boto.dynamodb2.connect_to_region(
                    args.region,
                    aws_access_key_id=aws_access_key_id,
                    aws_secret_access_key=aws_secret_access_key,
                    security_token=aws_security_token
                    )
                connect_try = False
            except Exception as e:
                log.error('Can\'t connect to DynamoDB, will retry: %s' % e.message)
                connect_try = True
                time.sleep(5)
        return _dynamoconn
    else:
        return _dynamoconn

def snsconn():
    """ This gets a sns connection """
    log = log_wrapper()
    # Create globals
    global _aws_expiration_token
    global _snsconn
    # Check existance of sns connection
    create_creds = False
    try:
        _snsconn.host
    except NameError:
        create_creds = True
        log.info('No SNS connection found, creating new one')
    # Check the token validity
    try:
        _aws_expiration_token
    except NameError:
        create_creds = True
    if not create_creds:
        if _aws_expiration_token != 0.0 and _aws_expiration_token - time.time() < 300:
            log.info('SNS connection expired, requesting new credentials')
            create_creds = True

    # If either is incorrect request new credentials and generate sns connection
    if create_creds:
        aws_access_key_id, aws_secret_access_key, aws_security_token, _aws_expiration_token = aws_authentication(args)
        log.debug('Creating DynamoDB connection')
        if config['dnsrefresher']['reload_sns_alert_endpoints'] or config['templates']['reload_sns_alert_endpoints'] or config['dnsrefresher']['reload_sns_notification_endpoints'] or config['templates']['reload_sns_notification_endpoints']:
            log.debug('Creating SNS connection')
            connect_try = False
            while connect_try:
                try:
                    _snsconn = boto.sns.connect_to_region(
                        args.region,
                        aws_access_key_id=aws_access_key_id,
                        aws_secret_access_key=aws_secret_access_key,
                        security_token=aws_security_token
                        )
                    connect_try = False
                except Exception as e:
                    log.error('Can\'t connect to SNS, will retry: %s' % e.message)
                    connect_try = True
                    time.sleep(5)
            return _snsconn
        else:
            return None
    else:
        return _snsconn

def dynamo_comparison_operator(dyn_key, dyn_comparison_operator):
    """ Builds a sem kwarg structure for comparison operator iterations """
    log = log_wrapper()
    # Checking dyn_key
    if dyn_key is None or dyn_key == '':
        log.error('Our key is right now empty, this is bad: %s' % dyn_key)
        raise SyntaxError
    # Build filter_kwarg and hack list to do IN trick
    if dyn_comparison_operator.lower() in ['eq', 'ne', 'le', 'lt', 'ge', 'gt', 'null', 'contains']:
        new_key = ("%s__%s" % (dyn_key, dyn_comparison_operator.lower()))
    elif dyn_comparison_operator.lower() in ['in', 'between']:
        # This is a special case as we don't want to iterate, its quicker to throw the whole array
        new_key = ("%s__%s" % (dyn_key, dyn_comparison_operator.lower()))
    elif dyn_comparison_operator.lower() == 'notnull' or dyn_comparison_operator.lower() == 'not_null':
        new_key = ("%s__notnull" % dyn_key)
    elif dyn_comparison_operator.lower() == 'notcontains' or dyn_comparison_operator.lower() == 'not_contains':
        new_key = ("%s__notcontains" % dyn_key)
    elif dyn_comparison_operator.lower() == 'beginswith' or dyn_comparison_operator.lower() == 'begins_with':
        new_key = ("%s__beginswith" % dyn_key)
    else:
        log.error('Unrecognised comparison operator %s' % dyn_comparison_operator)
    return new_key

def dynamo_scan(dyn_table,
                dyn_key=None,
                dyn_filter=[],
                dyn_comparison_operator=None):
    """ Runs a scan against dynamodb, captures results and ouputs data as dictionary """
    log = log_wrapper()
    dynamo_connection = dynamoconn()
    # Set table
    try:
        table = Table(dyn_table, connection=dynamo_connection)
    except Exception as e:
        log.error('Can\'t select table %s: %s' % (dyn_table, e.message))
        raise SystemError('Can\'t select table %s: %s', dyn_table, e.message)

    # Check if we have a filter or not
    if len(dyn_filter) > 0 and dyn_comparison_operator and dyn_key:
        log.debug('Using query with filtering key %s comparing %s with filter %s' % (dyn_key, dyn_comparison_operator, dyn_filter))
        use_filter = True
        filter_list = []

        if isinstance(dyn_filter, six.types.ListType):
            filter_list = dyn_filter
        elif isinstance(dyn_filter, six.types.StringTypes):
            filter_list.append(dyn_filter)
        else:
            log.error('Can\'t recognise filter %s' % dyn_filter)
            raise KeyError('Can\'t recognise filter %s', dyn_filter)

        # Build filter_kwarg
        new_key = dynamo_comparison_operator(dyn_key, dyn_comparison_operator)
        log.debug('Comparison operator is %s' % new_key)
        # Do IN trick and hack list
        if dyn_comparison_operator.lower() in ['in', 'between']:
            filter_list = dyn_filter.join('|')

    elif (dyn_filter is None and dyn_comparison_operator) or (dyn_filter and dyn_comparison_operator is None):
            log.error('Illegal filter structure, missing either filter or comparison operator')
    else:
        log.debug('Using query with no filtering')
        use_filter = False
        filter_list = list(['justmyiterator'])

    # Do the table scan and get results
    final_result = []
    for myfilter in filter_list:
        if use_filter:
            if '|' in myfilter:
                newfilter = myfilter.split('|')
                try:
                    result_set = table.scan(**{new_key: newfilter})
                except Exception as e:
                    log.error('Problem querying DynamoDB: %s' % e.message)
                    return None
            else:
                try:
                    result_set = table.scan(**{new_key: myfilter})
                except Exception as e:
                    log.error('Problem querying DynamoDB: %s' % e.message)
                    return None
        else:
            try:
                result_set = table.scan()
            except Exception as e:
                log.error('Problem querying DynamoDB: %s' % e.message)
                return None
        # Get data back from results
        for result in result_set:
            final_result.append(result._data)
    log.debug('Returning a result set of %s members from table %s' % (len(final_result), dyn_table))
    return final_result

def asg_resolve(name):
    """ Reads from our ASG stream and gets out a list of IP addresses """
    log = log_wrapper()
    real_name = name.split('.')[0]
    maintenance_list = []
    return_ips = []
    if os.path.exists(config['asg']['asg_file']):
        with open(config['asg']['asg_file'], 'r') as asg_file_read:
            asg_object = json_decode(asg_file_read.read())
            if asg_object is None:
                log.error('Can\'t decode json data from file, skipping for now')
                return []
        for asg_entry in asg_object:
            ip_array = eval(asg_entry[config['asg']['asg_value']])
            if len(ip_array) == 0:
                continue
            if asg_entry[config['asg']['asg_key']] == 'MAINTENANCE_MODE':
                maintenance_list = ip_array
            if asg_entry[config['asg']['asg_key']].lower() == real_name.lower():
                return_ips = ip_array
        # Remove maintenance IPs from return
        return_ips_filtered = [x for x in return_ips if x not in maintenance_list]
        return return_ips_filtered
    else:
        return []

def consul_resolve(name):
    """ Reads from our Consul stream and gets out a list of IP addresses """
    log = log_wrapper()
    real_name = name.split('.')[0]
    myreturn = []
    for mapping in config['consul']['consul_mappings']:
        consul_dc = mapping.split('|')[0]
        consul_filename = "%s/consul_%s.json" % (config['consul']['consul_file_dir'], consul_dc)
        if os.path.exists(consul_filename):
            with open(consul_filename, 'r') as consul_file_read:
                consul_object = json_decode(consul_file_read.read())
                if consul_object is None:
                    log.error('Can\'t decode json data from file, skipping for now')
                    return []
            for consul_entry in consul_object:
                if len(consul_entry['ip_array']) == 0:
                    continue
                if consul_entry['name'].lower() == real_name.lower():
                    myreturn = (consul_entry['ip_array'])
    return myreturn

def dns_resolve(name):
    """ Performs a DNS query and returns a sorted list of IPs """
    log = log_wrapper()
    answer_list = []
    # Just return an IP if it's an ip
    if re.match('^(?P<ipaddress>\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})$', name):
        answer_list.append(name)
        return answer_list
    # Look in Consul first
    consul_list = consul_resolve(name)
    if len(consul_list) == 0:
        # ASG table InfraAsgIPs content AsgName return IPs
        asg_list = asg_resolve(name)
        if len(asg_list) == 0:
            try:
                answers = dns.resolver.query(name, 'A')
            except Exception:
                answers = []
            for answer in answers:
                answer_list.append(answer.to_text())
            log.debug('Returning entry %s found in DNS' % name)
            return sorted(answer_list)
        else:
            log.debug('Returning entry %s found in ASG List' % name)
            return sorted(asg_list)
    else:
        log.debug('Returning entry %s found in Consul' % name)
        return sorted(consul_list)

def to_bool(value):
    """
       Converts 'something' to boolean. Raises exception for invalid formats
           Possible True  values: 1, True, "1", "TRue", "yes", "y", "t"
           Possible False values: 0, False, None, [], {}, "", "0", "faLse", "no", "n", "f", 0.0, ...
    """
    if str(value).lower() in ("yes", "y", "true", "t", "1"):
        return True
    if str(value).lower() in ("no", "n", "false", "f", "0", "0.0", "", "none", "[]", "{}"):
        return False
    raise Exception('Invalid value for boolean conversion: ' + str(value))

def to_list(value):
    """ Creates an array from any kind of object """
    initial_list = [x.strip() for x in value.translate(None, '!@#$[]{}\'"').split(',')]
    return [x for x in initial_list if x]

def json_encode(object):
    """ Encodes and returns a JSON stream """
    return simplejson.dumps(object)

def json_decode(string):
    """ Decodes a JSON stream and returns a python dictionary version """
    log = log_wrapper()
    try:
        decoded_json = simplejson.loads(string)
    except simplejson.JSONDecodeError:
        log.error('Can\'t decode JSON string: %s' % string)
        return None
    return decoded_json

def reload_program(command, max_tries=10):
    """ This function will reload a program, capture output and return the state and exec args """
    log = log_wrapper()
    reload_try = True
    tries = 0
    while reload_try:
        log.info('Reloading program %s' % command)
        myproc = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE)
        # Write state to file
        program_run_output = myproc.communicate()
        program_run_returncode = myproc.returncode
        log.info('Reload finished %s (%s)' % (command, program_run_returncode))
        if program_run_returncode == 0:
            reload_try = False
        else:
            if tries >= max_tries:
                reload_try = False
            else:
                tries += 1
            mysleep = random.randint(1, 10) * 60
            log.info('Will retry, sleeping for %s seconds' % mysleep)
            time.sleep(mysleep)
    return program_run_returncode, program_run_output

def template_thread_runner():
    """ Main thread runner for the templating engine """
    log = log_wrapper()
    try:
        # Sanitize entry
        if config is None:
            raise KeyError('Missing parameters')

        if not os.path.exists(config['templates']['destination_directory']):
            log.info('Template destination directory missing, will create')
            try:
                os.makedirs(config['templates']['destination_directory'])
            except:
                log.error('Cant create destination directory %s' % config['templates']['destination_directory'])
                raise BaseException('Can\'t create directory')

        # Loader for templates
        template_basedir = os.path.dirname(config['templates']['template_file'])
        log.debug('Template basedir is %s' % template_basedir)
        if not os.path.exists(template_basedir):
            log.error('Directory where templates should be in does not exist, this is not good')
            raise BaseException('Template directory not available, cannot continue')

        # Initialise jinja environment for our files
        jinja2_env = Environment(loader=FileSystemLoader(template_basedir), trim_blocks=False)
        # Add extra methods to template
        jinja2_env.globals.update(dns_resolve=dns_resolve)
        jinja2_env.globals.update(json_decode=json_decode)

        # Start iterator
        while True:
            log.debug('Starting iteration execution')
            # Evaluate template so we can use it to render files
            log.debug('Loading template %s' % os.path.basename(config['templates']['template_file']))
            try:
                template = jinja2_env.get_template(os.path.basename(config['templates']['template_file']))
            except Exception:
                log.error('Template has errors, please correct them:')
                continue
            ## Generate names and compare against target directory
            filename_list = []
            # Get query with list of things to go through
            log.debug('Querying DynamoDB table %s with key %s comparison operator %s and filter %s' % (config['dynamodb']['table'], config['dynamodb']['key'], config['dynamodb']['comparison_operator'], config['dynamodb']['filter_expression']))
            results = dynamo_scan(config['dynamodb']['table'],
                                  config['dynamodb']['key'],
                                  config['dynamodb']['filter_expression'],
                                  config['dynamodb']['comparison_operator'])
            if results is not None:
                # Recover results and iterate
                log.debug('Processing Data results')
                need_reload = False
                for result in results:
                    if config['templates']['data_field'] == "all":
                        result_data = result
                    else:
                        result_data = result[config['templates']['data_field']]
                    if config['templates']['json_decode']:
                        result_data = json_decode(result_data)
                        if result_data is None:
                            log.error('Can\'t decode data for file, skipping')
                            continue
                    # Render computed template
                    computed_template = template.render(**{'data': result_data})
                    # Build destination filename
                    filename_only_template = Environment().from_string(config['templates']['filename_pattern']).render(**{'data': result_data})
                    filename_list.append(filename_only_template)
                    # Compare rendered template to current file if it exists, decide to write or not
                    template_filename = "%s/%s" % (config['templates']['destination_directory'], filename_only_template)
                    write_file = True
                    if os.path.isfile(template_filename):
                        with open(template_filename, "r") as original_file:
                            orig_file_string = original_file.read()
                            if re.sub('[ \n]', '', orig_file_string) == re.sub('[ \n]', '', computed_template):
                                log.debug('File %s has not changed' % template_filename)
                                write_file = False
                            else:
                                log.info('File %s changed, refreshing' % template_filename)
                                # Gather upstream data here
                                for regenerated_host in result_data['Hosts']:
                                    dns_regenerated_host = dns_resolve(regenerated_host['DnsName'])
                                    if regenerated_host['State'].lower() == "down":
                                        regenerated_host_offline_status = " (offline)"
                                    else:
                                        regenerated_host_offline_status = ""
                                    log.info('File %s has host %s with %s upstreams%s' % (template_filename, regenerated_host['DnsName'], len(dns_regenerated_host), regenerated_host_offline_status))
                    if write_file:
                        # they're different, overwrite
                        log.info('Writing file %s' % template_filename)
                        with open(template_filename, "wb") as template_newfile:
                            template_newfile.write(computed_template)
                        need_reload = True

                ## Compare filename_list against our local directory and remove unnecessary files
                log.debug('Checking if we need to remove any files')
                local_directory = os.walk(config['templates']['destination_directory'])
                for root, dirnames, local_files in local_directory:
                    for local_file in local_files:
                        if local_file not in filename_list:
                            full_local_filename = "%s/%s" % (config['templates']['destination_directory'], local_file)
                            log.debug('Checking file %s' % full_local_filename)
                            # Remove this file as it shoudldn't be here
                            log.info('Removing file %s' % full_local_filename)
                            try:
                                os.remove(full_local_filename)
                            except OSError:
                                log.debug('Can\'t delete file %s, continuing' % full_local_filename)

                ## Reload process
                if need_reload and config['templates']['reload_command']:
                    log.info('Program needs reloading')
                    program_run_returncode, program_run_output = reload_program(config['templates']['reload_command'])
                    if program_run_returncode != 0 and config['templates']['reload_sns_alert_endpoints']:
                        sns_message = "Program failed to reload: %s\n%s" % (config['templates']['reload_command'], program_run_output)
                        for sns_endpoint in config['templates']['reload_sns_alert_endpoints']:
                            sns_connection = snsconn()
                            sns_connection.publish(topic=sns_endpoint, message=sns_message)

                    # SNS Alert when things are changed
                    if config['templates']['reload_sns_notification_endpoints']:
                        sns_message = "Program reloaded: %s" % config['templates']['reload_command']
                        for sns_endpoint in config['templates']['reload_sns_notification_endpoints']:
                            sns_connection = snsconn()
                            sns_connection.publish(topic=sns_endpoint, message=sns_message)

            ## Sleep our alloted time and start again
            log.debug('Sleeping for %s seconds' % config['templates']['refresh_rate'])
            time.sleep(int(config['templates']['refresh_rate']))
    except Exception as e:
        log.error('Caught exception in worker thread: %s' % e.message)
        raise e

def dns_refresher_thread_runner():
    """ This thread checks the created files and compares DNS entries against the current IPs, refreshes if needed """
    log = log_wrapper()
    try:
        while True:
            log.debug('Starting iteration execution')
            # Walk through files and check DNS values
            for root, dirnames, filenames in os.walk(config['dnsrefresher']['destination_directory']):
                for filename in filenames:
                    if re.match(config['dnsrefresher']['filename_filter'], filename):
                        # We got a valid file to treat
                        # Recover file and get matching DNS lines
                        dnsrefresh_filename = "%s/%s" % (config['dnsrefresher']['destination_directory'], filename)
                        log.debug('Checking file %s' % dnsrefresh_filename)
                        need_new_file = False
                        try:
                            with open(dnsrefresh_filename, "r") as dnsrefresh_file:
                                got_dns_name = False
                                ipaddress_pool = []
                                line_number = 0
                                for line in dnsrefresh_file:
                                    ### All line matching
                                    line_number += 1
                                    dns_line_pattern = re.match(config['dnsrefresher']['dns_line_filter'], line)
                                    ip_pattern = re.match('(.*)\ (?P<ipaddress>\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})(\ |:)(.*)', line)
                                    if ip_pattern and got_dns_name:
                                        ip_address = ip_pattern.group('ipaddress')
                                        valid_ip_address = True
                                        try:
                                            socket.inet_aton(ip_address)
                                        except socket.error:
                                            valid_ip_address = False
                                    elif dns_line_pattern:
                                        # We started a DNS block
                                        got_dns_name = True
                                        dns_name = dns_line_pattern.group('data')
                                    else:
                                        valid_ip_address = got_dns_name = False
                                    ### Follow up logic
                                    process_array = False
                                    if got_dns_name and valid_ip_address and (not dns_line_pattern):
                                        # Regular ip line to extract
                                        ipaddress_pool.append(ip_address)
                                    elif got_dns_name and dns_line_pattern and len(ipaddress_pool) > 0:
                                        # New DNS line pattern with previous array full, we need to process first
                                        process_array = True
                                    elif got_dns_name and (not valid_ip_address) and (not dns_line_pattern):
                                        # Unknown line after a block of IPs
                                        process_array = True
                                    elif got_dns_name and valid_ip_address and dns_line_pattern:
                                        # We got a weird line with both valid IPs and valid DNS matching, DNS wins
                                        got_dns_name = True
                                    ### Process array
                                    if process_array:
                                        # Resolve IP address to array
                                        dns_ip_group = dns_resolve(dns_name)
                                        # Compare arrays
                                        if dns_ip_group.sort() == ipaddress_pool.sort():
                                            # Both match, we're happy and good
                                            log.debug('IP groups match, nothing to do')
                                        else:
                                            need_new_file = True
                        except EnvironmentError:
                            log.debug('File %s dissappeared while reading' % dnsrefresh_filename)
                            continue

                        # Write a new file to StringIO then to the real file once we replaced all the necessary values
                        if need_new_file:
                            log.info('Refreshing file %s due to DNS change' % dnsrefresh_filename)
                            # Create new file object
                            new_upstream_file = StringIO.StringIO()
                            # Iterate through lines and replace
                            with open(dnsrefresh_filename, "r") as dnsrefresh_newfile:
                                newlines = 0
                                ip_replacement_inprogress = False
                                for newcontent in dnsrefresh_newfile:
                                    newlines += 1
                                    dns_line_pattern = re.match(config['dnsrefresher']['dns_line_filter'], newcontent)
                                    ip_pattern = re.match('(.*)\ (?P<ipaddress>\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})(\ |:)(.*)', newcontent)
                                    if dns_line_pattern:
                                        dns_name = dns_line_pattern.group('data')
                                        dns_ip_group = dns_resolve(dns_name)
                                        ip_replacement_inprogress = True
                                        initial_ipaddress_block_line = newlines + 1
                                        with open(new_upstream_file, "a") as new_upstream_file_line:
                                            new_upstream_file_line.write(newcontent)
                                    elif ip_replacement_inprogress and ip_pattern:
                                        if initial_ipaddress_block_line == newlines:
                                            for new_upstream_ip in dns_ip_group:
                                                new_upstream_ip_content = re.sub('\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}', new_upstream_ip, newcontent)
                                                with open(new_upstream_file, "a") as new_upstream_file_line:
                                                    new_upstream_file_line.write(new_upstream_ip_content)
                                    else:
                                        ip_replacement_inprogress = False
                                        with open(new_upstream_file, "a") as new_upstream_file_line:
                                            new_upstream_file_line.write(newcontent)

                            # We finished iterating through StringIO, now write it down as the real thing
                            with open(dnsrefresh_filename, 'w') as dnsrefresh_writefile:
                                fcntl.flock(dnsrefresh_writefile, fcntl.LOCK_EX)
                                dnsrefresh_writefile.write(new_upstream_file)
                                new_upstream_file.close()

                            ## Reload process
                            if config['dnsrefresher']['reload_command']:
                                log.info('Program needs reloading')
                                program_run_returncode, program_run_output = reload_program(config['dnsrefresher']['reload_command'])
                                if program_run_returncode != 0 and config['dnsrefresher']['reload_sns_alert_endpoints']:
                                    sns_message = "Program failed to reload: %s\n%s" % (config['dnsrefresher']['reload_command'], program_run_output)
                                    for sns_endpoint in config['dnsrefresher']['reload_sns_alert_endpoints']:
                                        sns_connection = snsconn()
                                        sns_connection.publish(topic=sns_endpoint, message=sns_message)
                                # SNS Alert when things are changed
                                if config['dnsrefresher']['reload_sns_notification_endpoints']:
                                    sns_message = "Program reloaded: %s" % config['dnsrefresher']['reload_command']
                                    for sns_endpoint in config['dnsrefresher']['reload_sns_notification_endpoints']:
                                        sns_connection = snsconn()
                                        sns_connection.publish(topic=sns_endpoint, message=sns_message)

            log.debug('Sleeping for %s seconds' % config['dnsrefresher']['refresh_rate'])
            time.sleep(int(config['dnsrefresher']['refresh_rate']))
    except Exception as e:
        log.error('Caught exception in worker thread: %s' % e.message)
        raise e

def asg_refresher_thread_runner():
    """ ASG Table refresher """
    log = log_wrapper()
    try:
        if config['asg']['use_asg_dynamodb']:
            while True:
                log.debug('Starting iteration execution')
                log.debug('Scanning table InfraAsgIPs')
                results = dynamo_scan('InfraAsgIPs')
                if results is not None:
                    log.debug('Converting results to JSON')
                    json_results = json_encode(results)
                    log.debug('Writing file %s' % config['asg']['asg_file'])
                    with open(config['asg']['asg_file'], 'w') as asg_file:
                        fcntl.flock(asg_file, fcntl.LOCK_EX)
                        asg_file.write(json_results)
                else:
                    log.debug('No results found in table InfraAsgIPs')
                log.debug('Sleeping for %s seconds' % config['asg']['refresh_rate'])
                time.sleep(int(config['asg']['refresh_rate']))
    except Exception as e:
        log.error('Caught exception in worker thread: %s' % e.message)
        raise e

def consul_refresher_thread_runner():
    """ Consul refresher """
    log = log_wrapper()
    try:
        if config['consul']['use_consul']:
            while True:
                log.debug('Starting iteration execution')
                log.debug('Received mappings: %s' % config['consul']['consul_mappings'])
                for mapping in config['consul']['consul_mappings']:
                    log.debug('Processing mapping: %s' % mapping)
                    consul_results = []
                    if len(mapping.split('|')) == 4:
                        consul_dc = mapping.split('|')[0]
                        consul_envs = mapping.split('|')[1].split(';')
                        consul_servers = mapping.split('|')[2].split(';')
                        consul_token = mapping.split('|')[3]
                    elif len(mapping.split('|')) == 3:
                        consul_dc = mapping.split('|')[0]
                        consul_envs = mapping.split('|')[1].split(';')
                        consul_servers = mapping.split('|')[2].split(';')
                        consul_token = None
                    else:
                        log.debug('Mapping is not valid')
                        # Mapping is not valid, skipping
                        continue
                    log.debug('Consul mappings for env %s with servers %s and token %s' % (consul_envs, consul_servers, consul_token))
                    consul_need_info = True
                    for consul_server in consul_servers:
                        if consul_need_info:
                            log.debug('Connecting to server %s with token %s' % (consul_server, consul_token))
                            params = {}
                            consul_url_base = "http://%s:8500" % consul_server
                            if consul_token:
                                params.update({'token': consul_token})
                            # Get list of services
                            consul_url_services = "%s/v1/catalog/services" % consul_url_base
                            try:
                                consul_services_raw = requests.get(consul_url_services, params=params)
                            except Exception as e:
                                log.debug('Cant connect to Consul: %s' % e.message)
                                continue
                            try:
                                consul_services = simplejson.loads(consul_services_raw._content)
                            except simplejson.JSONDecodeError as e:
                                log.debug('JSON Decode error: %s' % e.message)
                                continue
                            # At this point we dont care if we get info back or not, we're okay
                            consul_need_info = False
                            log.debug('Reported consul services: %s' % consul_services)
                            # Iterate through services and get IPs
                            for consul_service in consul_services:
                                # Check that it matches our environments so we only match services that are of interest to us
                                log.debug('Checking matches for consul service %s' % consul_service)
                                for consul_env in consul_envs:
                                    if consul_service.startswith(consul_env):
                                        log.debug('MATCH FOUND for consul service %s' % consul_service)
                                        consul_ip_array = []
                                        consul_health_service_url = "%s/v1/health/service/%s" % (consul_url_base, consul_service)
                                        try:
                                            consul_health_services_raw = requests.get(consul_health_service_url, params=params)
                                        except Exception as e:
                                            continue
                                        try:
                                            consul_health_services = simplejson.loads(consul_health_services_raw._content)
                                        except simplejson.JSONDecodeError:
                                            continue
                                        for consul_health_service in consul_health_services:
                                            consul_service_health = True
                                            try:
                                                if consul_health_service['Service']['Address'] is not '127.0.0.1':
                                                    consul_service_node_ip = consul_health_service['Service']['Address']
                                                else:
                                                    log.info('Service %s with incorrect IP definition 127.0.0.1 in Service' % consul_service)
                                                    continue
                                            except:
                                                try:
                                                    if consul_health_service['Node']['Address'] is not '127.0.0.1':
                                                        consul_service_node_ip = consul_health_service['Node']['Address']
                                                    else:
                                                        log.info('Service %s with incorrect IP definition 127.0.0.1 in Node' % consul_service)
                                                        continue
                                                except:
                                                    continue
                                            for health_check in consul_health_service['Checks']:
                                                if health_check['Status'] == 'critical':
                                                    consul_service_health = False
                                                    log.debug('Service check critical on node %s' % consul_service_node_ip)
                                            if consul_service_health:
                                                consul_ip_array.append(consul_service_node_ip)
                                        consul_results.append({ 'name': consul_service, 'ip_array': consul_ip_array })
                                        log.debug('Publishing service %s with IPs %s to array %s' % (consul_service, consul_ip_array, consul_results))
                    # Write consul results file
                    if len(consul_results) > 0:
                        json_results = json_encode(consul_results)
                        consul_filename = "%s/consul_%s.json" % (config['consul']['consul_file_dir'], consul_dc)
                        log.debug('Writing file %s' % consul_filename)
                        log.debug('%s - Got %s results' % (consul_dc, len(consul_results)))
                        with open(consul_filename, 'w') as consul_file:
                            fcntl.flock(consul_file, fcntl.LOCK_EX)
                            consul_file.write(json_results)
                    else:
                        log.debug('Nothing to write for DC %s, either Consul is empty or there is a problem' % consul_dc)
                log.debug('Sleeping for %s seconds' % config['consul']['refresh_rate'])
                time.sleep(int(config['consul']['refresh_rate']))
    except Exception as e:
        log.error('Caught exception in worker thread: %s' % e.message)
        raise e


def main(argv):
    """ Main program """
    # Populate env vars (if available)
    try:
        aws_access = os.environ['AWS_ACCESS_KEY_ID']
        aws_key = os.environ['AWS_SECRET_ACCESS_KEY']
    except KeyError:
        aws_access = aws_key = False

    # Parse args
    parser = argparse.ArgumentParser()
    parser.add_argument('-p', '--profile', '--aws_profile', help="Recover credentials from IAM profile", action='store_true', default=False)
    parser.add_argument('-r', '--region', help="AWS Region", default="eu-west-1")
    parser.add_argument('-c', '--config', help="Upstreamr config", default="/etc/upstreamr/upstreamr.conf")
    parser.add_argument('-d', '--debug', help="Debug mode", action='store_true', default=False)
    parser.add_argument('-v', '--verbose', help="Show logs through console", action='store_true', default=False)
    parser.add_argument('-l', '--log', help="Logfile location", default="/var/log/upstreamr.log")
    parser.add_argument('--aws_access_key', help="AWS Access Key", default=aws_access)
    parser.add_argument('--aws_secret_key', help="AWS Secret Key", default=aws_key)
    global args
    args = parser.parse_args()

    # Enable logging facilities
    if args.debug:
        loglevel = 'DEBUG'
    else:
        loglevel = 'INFO'
    # Initialise logger
    console_loglevel = logfile_loglevel = logging.getLevelName(loglevel)
    global logger
    logger = logging.getLogger('upstreamr')
    logger.setLevel(logging.DEBUG)
    # create file handler
    logfile_handler = logging.handlers.RotatingFileHandler(args.log, mode='a', maxBytes=500000, backupCount=5)
    logfile_handler.setLevel(logfile_loglevel)
    # create console handler with a higher log level
    consolehandler = logging.StreamHandler()
    consolehandler.setLevel(console_loglevel)
    # create formatter and add it to the handlers
    formatter = logging.Formatter(
        '%(asctime)s %(name)s %(levelname)s %(message)s', '%b %e %H:%M:%S')
    logfile_handler.setFormatter(formatter)
    consolehandler.setFormatter(formatter)
    # add the handlers to the logger
    if args.verbose:
        logger.addHandler(consolehandler)
    logger.addHandler(logfile_handler)

    log = log_wrapper()
    log.info('Starting Upstreamr')
    log.debug('Created logger facilities')

    unparsed_config = ConfigParser.SafeConfigParser(allow_no_value=True)
    try:
        with open(args.config):
            unparsed_config.read(args.config)
    except Exception as e:
        log.critical('Can\'t read config file: %s' % args.config)
        sys.exit(1)

    ### Sanitise config values and adopt default values
    ## Check that sections exist
    log.debug('Sanitising input config')
    for section in ['asg', 'dynamodb', 'templates', 'dnsrefresher']:
        if not unparsed_config.has_section(section):
            unparsed_config.add_section(section)

    ## Check that critical options are in place, otherwise throw an error
    # Dynamodb section
    for dynamodb_option in ['table', 'key', 'filter_expression']:
        if not unparsed_config.has_option('dynamodb', dynamodb_option):
            raise ConfigParser.NoOptionError(dynamodb_option, 'dynamodb')
    # ASG section
    if unparsed_config.get('asg', 'use_asg_dynamodb') is True:
        for dynamodb_asg_option in ['asg_table', 'asg_key', 'asg_value']:
            if not unparsed_config.has_option('asg', dynamodb_asg_option):
                raise ConfigParser.NoOptionError(dynamodb_asg_option, 'asg')
    # Templates section
    for templates_option in ['destination_directory', 'template_file', 'data_field', 'filename_pattern']:
        if not unparsed_config.has_option('templates', templates_option):
            raise ConfigParser.NoOptionError(templates_option, 'templates')
    # DNSRefresher section
    for dnsrefresher_option in ['destination_directory']:
        if not unparsed_config.has_option('dnsrefresher', dnsrefresher_option):
            raise ConfigParser.NoOptionError(dnsrefresher_option, 'dnsrefresher')

    ## Fill not critical values if they're empty or unexisting
    if not (unparsed_config.has_option('templates', 'refresh_rate') or unparsed_config.get('templates', 'refresh_rate') == ''):
        unparsed_config.set('templates', 'refresh_rate', 30)
    if not (unparsed_config.has_option('dynamodb', 'comparison_operator') or unparsed_config.get('dynamodb', 'comparison_operator') == ''):
        unparsed_config.set('dynamodb', 'comparison_operator', 'contains')
    if not (unparsed_config.has_option('asg', 'use_asg_dynamodb') or unparsed_config.get('asg', 'use_asg_dynamodb') == ''):
        unparsed_config.set('asg', 'use_asg_dynamodb', False)
    if not (unparsed_config.has_option('asg', 'asg_file') or unparsed_config.get('asg', 'asg_file') == ''):
        unparsed_config.set('asg', 'asg_file', '/tmp/asg_upstreamr.json')
    if not (unparsed_config.has_option('asg', 'refresh_rate') or unparsed_config.get('asg', 'refresh_rate') == ''):
        unparsed_config.set('asg', 'refresh_rate', 20)
    if not (unparsed_config.has_option('consul', 'use_consul') or unparsed_config.get('consul', 'use_consul') == ''):
        unparsed_config.set('consul', 'use_consul', False)
    if not (unparsed_config.has_option('consul', 'consul_file_dir') or unparsed_config.get('consul', 'consul_file_dir') == ''):
        unparsed_config.set('consul', 'consul_file_dir', '/tmp')
    if not (unparsed_config.has_option('consul', 'refresh_rate') or unparsed_config.get('consul', 'refresh_rate') == ''):
        unparsed_config.set('consul', 'refresh_rate', 5)
    if not (unparsed_config.has_option('templates', 'json_decode') or unparsed_config.get('templates', 'json_decode') == ''):
        unparsed_config.set('templates', 'json_decode', False)
    if not (unparsed_config.has_option('templates', 'reload_sns_alert_endpoints') or unparsed_config.get('templates', 'reload_sns_alert_endpoints') == ''):
        unparsed_config.set('templates', 'reload_sns_alert_endpoints', None)
    if not (unparsed_config.has_option('templates', 'reload_sns_notification_endpoints') or unparsed_config.get('templates', 'reload_sns_notification_endpoints') == ''):
        unparsed_config.set('templates', 'reload_sns_notification_endpoints', None)
    if not (unparsed_config.has_option('templates', 'reload_command') or unparsed_config.get('templates', 'reload_command') == ''):
        unparsed_config.set('templates', 'reload_command', None)
    if not (unparsed_config.has_option('dnsrefresher', 'reload_sns_alert_endpoints') or unparsed_config.get('dnsrefresher', 'reload_sns_alert_endpoints') == ''):
        unparsed_config.set('dnsrefresher', 'reload_sns_alert_endpoints', None)
    if not (unparsed_config.has_option('dnsrefresher', 'reload_sns_notification_endpoints') or unparsed_config.get('dnsrefresher', 'reload_sns_notification_endpoints') == ''):
        unparsed_config.set('dnsrefresher', 'reload_sns_notification_endpoints', None)
    if not (unparsed_config.has_option('dnsrefresher', 'reload_command') or unparsed_config.get('dnsrefresher', 'reload_command') == ''):
        unparsed_config.set('dnsrefresher', 'reload_command', None)
    if not (unparsed_config.has_option('dnsrefresher', 'refresh_rate') or unparsed_config.get('dnsrefresher', 'refresh_rate') == ''):
        unparsed_config.set('dnsrefresher', 'refresh_rate', 30)

    # Write finished config
    # pylint: disable=W0212
    global config
    log.debug('Created parsed config global')
    config = unparsed_config._sections

    # Bool converter
    config['asg']['use_asg_dynamodb'] = to_bool(config['asg']['use_asg_dynamodb'])
    config['consul']['use_consul'] = to_bool(config['consul']['use_consul'])
    config['templates']['json_decode'] = to_bool(config['templates']['json_decode'])
    config['dnsrefresher']['enable'] = to_bool(config['dnsrefresher']['enable'])
    # Array converter
    config['dynamodb']['filter_expression'] = to_list(config['dynamodb']['filter_expression'])
    config['consul']['consul_mappings'] = to_list(config['consul']['consul_mappings'])
    config['templates']['reload_sns_notification_endpoints'] = to_list(config['templates']['reload_sns_notification_endpoints'])
    config['templates']['reload_sns_alert_endpoints'] = to_list(config['templates']['reload_sns_alert_endpoints'])
    config['dnsrefresher']['reload_sns_notification_endpoints'] = to_list(config['dnsrefresher']['reload_sns_notification_endpoints'])
    config['dnsrefresher']['reload_sns_alert_endpoints'] = to_list(config['dnsrefresher']['reload_sns_alert_endpoints'])

    # Launch process threads
    template_runner = multiprocessing.Process(name='template_runner', target=template_thread_runner)
    asg_refresher_runner = multiprocessing.Process(name='asg_refresher_runner', target=asg_refresher_thread_runner)
    consul_refresher_runner = multiprocessing.Process(name='consul_refresher_runner', target=consul_refresher_thread_runner)
    dns_refresher_runner = multiprocessing.Process(name='dns_refresher_runner', target=dns_refresher_thread_runner)

    running_processes = []
    # Launch processes
    if config['dnsrefresher']['enable']:
        log.info('Starting DNS Refresher thread')
        dns_refresher_runner.daemon = True
        dns_refresher_runner.start()
        running_processes.append(dns_refresher_runner)
    else:
        log.info('No dns runner')
    if config['asg']['use_asg_dynamodb']:
        log.info('Starting ASG Table Refresher thread')
        asg_refresher_runner.daemon = True
        asg_refresher_runner.start()
        running_processes.append(asg_refresher_runner)
    else:
        log.info('No asg runner')
    if config['consul']['use_consul']:
        log.info('Starting Consul Refresher thread')
        consul_refresher_runner.daemon = True
        consul_refresher_runner.start()
        running_processes.append(consul_refresher_runner)
    else:
        log.info('No consul runner')
    # I want to sleep a bit before I start the template engine so ASG can populate
    log.info('Starting template engine')
    template_runner.daemon = True
    template_runner.start()
    running_processes.append(template_runner)

    # Check that we keep things alive, otherwise kick them switfly
    while True:
        time.sleep(5)
        for process in running_processes:
            log.debug('ProcessWatchDog - Checking process %s' % process.name)
            restart_process = False
            log.debug('Process %s is %s and exitcode %s' % (process.name, process.is_alive(), process.exitcode))
            if process.exitcode is None and not process.is_alive():
                log.info('ProcessWatchDog - Process %s didn\'t finish and is not running, restarting' % process.name)
                restart_process = True
            elif process.exitcode is not None and process.exitcode is not 0:
                log.info('ProcessWatchDog - Process %s ended with an error code %s, restarting' % (process.name, process.exitcode))
                restart_process = True
            if restart_process:
                log.info('Terminating process: %s' % process.name)
                process.terminate()
                running_processes.remove(process)
                if process.name == 'template_runner':
                    log.info('Starting new process template_runner')
                    template_runner = multiprocessing.Process(name='template_runner', target=template_thread_runner)
                    template_runner.daemon = True
                    template_runner.start()
                    running_processes.append(template_runner)
                elif process.name == 'asg_refresher_runner':
                    log.info('Starting new process asg_refresher_runner')
                    asg_refresher_runner = multiprocessing.Process(name='asg_refresher_runner', target=asg_refresher_thread_runner)
                    asg_refresher_runner.daemon = True
                    asg_refresher_runner.start()
                    running_processes.append(asg_refresher_runner)
                elif process.name == 'consul_refresher_runner':
                    log.info('Starting new process consul_refresher_runner')
                    consul_refresher_runner = multiprocessing.Process(name='consul_refresher_runner', target=consul_refresher_thread_runner)
                    consul_refresher_runner.daemon = True
                    consul_refresher_runner.start()
                    running_processes.append(consul_refresher_runner)
                elif process.name == 'dns_refresher_runner':
                    log.info('Starting new process dns_refresher_runner')
                    dns_refresher_runner = multiprocessing.Process(name='dns_refresher_runner', target=dns_refresher_thread_runner)
                    dns_refresher_runner.daemon = True
                    dns_refresher_runner.start()
                    running_processes.append(dns_refresher_runner)

if __name__ == "__main__":
    main(sys.argv[1:])
