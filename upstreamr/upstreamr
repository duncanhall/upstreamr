#!/usr/bin/env python
# vim: tabstop=4 expandtab shiftwidth=4 softtabstop=4

import os
import sys
import fcntl
import argparse
import time
import calendar
import logging
import re
import multiprocessing
import dns.resolver
import six
from six.moves import configparser
import requests
from jinja2 import Environment, FileSystemLoader
import boto.sns
import boto.dynamodb2
from boto.dynamodb2.table import Table
from environment_manager.api import EMApi
from environment_manager.utils import LogWrapper, to_bool, to_list, json_encode, json_decode, reload_program, json_load_file

# Define global vars
_DYNAMOCONN = None
_SNSCONN = None
_AWS_EXPIRATION_TOKEN = None
CONFIG = None
ARGS = None

def aws_authentication(credentials):
    """ Perform AWS authentication, preferred choice is IAM Role """
    log = LogWrapper()
    log.info('Requesting AWS credentials')
    # Connect to AWS
    # Connect through the inherited IAM role if we have one
    aws_access_key_id = False
    aws_secret_access_key = False
    # If role is not available select them from vars or command line
    if credentials.profile:
        sts_connection = boto.connect_sts()
        instance_profile_arn = boto.utils.get_instance_metadata()['iam']['info']['InstanceProfileArn'].replace('instance-profile', 'role')
        try:
            assumed_role_object = sts_connection.assume_role(
                instance_profile_arn,
                role_session_name="Upstreamr"
            )
        except Exception as error:
            log.critical('Can\'t assume role through STS, exiting: %s' % error.message)
            sys.exit(1)
        log.info('AWS Authenticated through instance profile %s' % instance_profile_arn)
        aws_access_key_id = assumed_role_object.credentials.access_key
        aws_secret_access_key = assumed_role_object.credentials.secret_key
        aws_security_token = assumed_role_object.credentials.session_token
        aws_expiration_token = calendar.timegm(time.strptime(assumed_role_object.credentials.expiration, "%Y-%m-%dT%H:%M:%SZ"))
        log.debug('Credential Expiration in: %s' % assumed_role_object.credentials.expiration)
    else:
        log.info('AWS Authenticated using command line arguments')
        aws_access_key_id = credentials.aws_access_key
        aws_secret_access_key = credentials.aws_secret_key
        aws_security_token = None
        aws_expiration_token = 0.0

    # Check I've got credentials
    if aws_access_key_id and aws_secret_access_key:
        pass
    else:
        log.critical('We couldnt find any AWS credentials, exiting...')
        sys.exit(1)
    return aws_access_key_id, aws_secret_access_key, aws_security_token, aws_expiration_token

def dynamoconn():
    """ This gets a dynamodb connection """
    log = LogWrapper()
    # Define globals
    global _AWS_EXPIRATION_TOKEN
    global _DYNAMOCONN
    # Check existance of dynamoconn
    create_creds = False
    try:
        _DYNAMOCONN.host
    except (NameError, AttributeError):
        log.info('No DynamoDB connection found, creating new one')
        create_creds = True
    # Check the token validity
    try:
        _AWS_EXPIRATION_TOKEN
    except (NameError, AttributeError):
        create_creds = True
    if not create_creds:
        if _AWS_EXPIRATION_TOKEN != 0.0 and _AWS_EXPIRATION_TOKEN - time.time() < 300.0:
            log.info('DynamoDB connection expired, requesting new credentials')
            create_creds = True

    # If either is incorrect request new credentials and generate dynamoconn
    if create_creds:
        aws_access_key_id, aws_secret_access_key, aws_security_token, _AWS_EXPIRATION_TOKEN = aws_authentication(ARGS)
        log.debug('Creating DynamoDB connection')
        connect_try = True
        while connect_try:
            try:
                _DYNAMOCONN = boto.dynamodb2.connect_to_region(
                    ARGS.region,
                    aws_access_key_id=aws_access_key_id,
                    aws_secret_access_key=aws_secret_access_key,
                    security_token=aws_security_token
                    )
                connect_try = False
            except Exception as error:
                log.error('Can\'t connect to DynamoDB, will retry: %s' % error.message)
                connect_try = True
                time.sleep(5)
        return _DYNAMOCONN
    else:
        return _DYNAMOCONN

def snsconn():
    """ This gets a sns connection """
    log = LogWrapper()
    # Create globals
    global _AWS_EXPIRATION_TOKEN
    global _SNSCONN
    # Check existance of sns connection
    create_creds = False
    try:
        _SNSCONN.host
    except (NameError, AttributeError):
        create_creds = True
        log.info('No SNS connection found, creating new one')
    # Check the token validity
    try:
        _AWS_EXPIRATION_TOKEN
    except (NameError, AttributeError):
        create_creds = True
    if not create_creds:
        if _AWS_EXPIRATION_TOKEN != 0.0 and _AWS_EXPIRATION_TOKEN - time.time() < 300:
            log.info('SNS connection expired, requesting new credentials')
            create_creds = True

    # If either is incorrect request new credentials and generate sns connection
    if create_creds:
        aws_access_key_id, aws_secret_access_key, aws_security_token, _AWS_EXPIRATION_TOKEN = aws_authentication(ARGS)
        log.debug('Creating DynamoDB connection')
        if CONFIG['templates']['reload_sns_alert_endpoints'] or CONFIG['templates']['reload_sns_notification_endpoints']:
            log.debug('Creating SNS connection')
            connect_try = False
            while connect_try:
                try:
                    _SNSCONN = boto.sns.connect_to_region(
                        ARGS.region,
                        aws_access_key_id=aws_access_key_id,
                        aws_secret_access_key=aws_secret_access_key,
                        security_token=aws_security_token
                        )
                    connect_try = False
                except Exception as error:
                    log.error('Can\'t connect to SNS, will retry: %s' % error.message)
                    connect_try = True
                    time.sleep(5)
            return _SNSCONN
        else:
            return None
    else:
        return _SNSCONN

def dynamo_comparison_operator(dyn_key, dyn_comparison_operator):
    """ Build a sem kwarg structure for comparison operator iterations """
    log = LogWrapper()
    # Checking dyn_key
    if dyn_key is None or dyn_key == '':
        log.error('Our key is right now empty, this is bad: %s' % dyn_key)
        raise SyntaxError
    # Build filter_kwarg and hack list to do IN trick
    if dyn_comparison_operator.lower() in ['eq', 'ne', 'le', 'lt', 'ge', 'gt', 'null', 'contains']:
        new_key = ("%s__%s" % (dyn_key, dyn_comparison_operator.lower()))
    elif dyn_comparison_operator.lower() in ['in', 'between']:
        # This is a special case as we don't want to iterate, its quicker to throw the whole array
        new_key = ("%s__%s" % (dyn_key, dyn_comparison_operator.lower()))
    elif dyn_comparison_operator.lower() == 'notnull' or dyn_comparison_operator.lower() == 'not_null':
        new_key = ("%s__notnull" % dyn_key)
    elif dyn_comparison_operator.lower() == 'notcontains' or dyn_comparison_operator.lower() == 'not_contains':
        new_key = ("%s__notcontains" % dyn_key)
    elif dyn_comparison_operator.lower() == 'beginswith' or dyn_comparison_operator.lower() == 'begins_with':
        new_key = ("%s__beginswith" % dyn_key)
    else:
        log.error('Unrecognised comparison operator %s' % dyn_comparison_operator)
    return new_key

def dynamo_scan(dyn_table,
                dyn_key=None,
                dyn_filter=[],
                dyn_comparison_operator=None):
    """ Run a scan against dynamodb, captures results and ouputs data as dictionary """
    log = LogWrapper()
    dynamo_connection = dynamoconn()
    # Set table
    try:
        table = Table(dyn_table, connection=dynamo_connection)
    except Exception as error:
        log.error('Can\'t select table %s: %s' % (dyn_table, error.message))
        raise SystemError('Can\'t select table %s: %s', dyn_table, error.message)

    # Check if we have a filter or not
    if len(dyn_filter) > 0 and dyn_comparison_operator and dyn_key:
        log.debug('Using query with filtering key %s comparing %s with filter %s' % (dyn_key, dyn_comparison_operator, dyn_filter))
        use_filter = True
        filter_list = []

        if isinstance(dyn_filter, six.types.ListType):
            filter_list = dyn_filter
        elif isinstance(dyn_filter, six.types.StringTypes):
            filter_list.append(dyn_filter)
        else:
            log.error('Can\'t recognise filter %s' % dyn_filter)
            raise KeyError('Can\'t recognise filter %s', dyn_filter)

        # Build filter_kwarg
        new_key = dynamo_comparison_operator(dyn_key, dyn_comparison_operator)
        log.debug('Comparison operator is %s' % new_key)
        # Do IN trick and hack list
        if dyn_comparison_operator.lower() in ['in', 'between']:
            filter_list.append('|'.join(dyn_filter))
        elif (dyn_filter is None and dyn_comparison_operator) or (dyn_filter and dyn_comparison_operator is None):
            log.error('Illegal filter structure, missing either filter or comparison operator')
    else:
        log.debug('Using query with no filtering')
        use_filter = False
        filter_list = list(['justmyiterator'])

    # Do the table scan and get results
    final_result = []
    for myfilter in filter_list:
        if use_filter:
            if '|' in myfilter:
                newfilter = myfilter.split('|')
                try:
                    result_set = table.scan(**{new_key: newfilter})
                except Exception as error:
                    log.error('Problem querying DynamoDB: %s' % error.message)
                    return None
            else:
                try:
                    result_set = table.scan(**{new_key: myfilter})
                except Exception as error:
                    log.error('Problem querying DynamoDB: %s' % error.message)
                    return None
        else:
            try:
                result_set = table.scan()
            except Exception as error:
                log.error('Problem querying DynamoDB: %s' % error.message)
                return None
        # Get data back from results
        for result in result_set:
            final_result.append(result._data)
    log.debug('Returning a result set of %s members from table %s' % (len(final_result), dyn_table))
    return final_result

def asg_resolve(name):
    """ Reads from our ASG stream and gets out a list of IP addresses """
    log = LogWrapper()
    real_name = name.split('.')[0]
    maintenance_list = []
    return_ips = []
    if os.path.exists(CONFIG['asg']['asg_file']):
        try:
            asg_object = json_load_file(CONFIG['asg']['asg_file'])
        except Exception as error:
            log.debug('Cannot find result for asg_resolve: %s' % error.message )
            return []
        for asg_entry in asg_object:
            ip_array = eval(asg_entry[CONFIG['asg']['asg_value']])
            if len(ip_array) == 0:
                continue
            if asg_entry[CONFIG['asg']['asg_key']] == 'MAINTENANCE_MODE':
                maintenance_list = ip_array
            if asg_entry[CONFIG['asg']['asg_key']].lower() == real_name.lower():
                return_ips = ip_array
        # Remove maintenance IPs from return
        return_ips_filtered = [x for x in return_ips if x not in maintenance_list]
        return return_ips_filtered
    else:
        return []

def consul_resolve(name):
    """ Reads from our Consul stream and gets out a list of IP addresses """
    log = LogWrapper()
    real_name = name.split('.')[0]
    myreturn = []
    for mapping in CONFIG['consul']['consul_mappings']:
        consul_dc = mapping.split('|')[0]
        consul_filename = "%s/consul_%s.json" % (CONFIG['consul']['file_dir'], consul_dc)
        if os.path.exists(consul_filename):
            try:
                consul_object = json_load_file(consul_filename)
            except Exception as error:
                log.debug('Cannot find result for consul_resolve: %s' % error.message )
                return []
            for consul_entry in consul_object:
                if len(consul_entry['ip_array']) == 0:
                    continue
                if consul_entry['name'].lower() == real_name.lower():
                    myreturn = (consul_entry['ip_array'])
    return myreturn

def dns_resolve(name):
    """ Performs a DNS query and returns a sorted list of IPs """
    log = LogWrapper()
    answer_list = []
    # Just return an IP if it's an ip
    if re.match('^(?P<ipaddress>\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})$', name):
        answer_list.append(name)
        return answer_list
    # Look in Consul first
    consul_list = consul_resolve(name)
    if len(consul_list) == 0:
        # ASG table InfraAsgIPs content AsgName return IPs
        asg_list = asg_resolve(name)
        if len(asg_list) == 0:
            try:
                answers = dns.resolver.query(name, 'A')
            except Exception:
                answers = []
            for answer in answers:
                answer_list.append(answer.to_text())
            log.debug('Returning entry %s found in DNS' % name)
            return sorted(answer_list)
        else:
            log.debug('Returning entry %s found in ASG List' % name)
            return sorted(asg_list)
    else:
        log.debug('Returning entry %s found in Consul' % name)
        return sorted(consul_list)

def template_thread_runner():
    """ Main thread runner for the templating engine """
    log = LogWrapper()
    # Enable debug if needed
    if CONFIG['templates']['debug']:
        logging.getLogger().setLevel(logging.DEBUG)
    try:
        # Sanitize entry
        if CONFIG is None:
            raise KeyError('Missing parameters')

        if not os.path.exists(CONFIG['templates']['destination_directory']):
            log.info('Template destination directory missing, will create')
            try:
                os.makedirs(CONFIG['templates']['destination_directory'])
            except:
                log.error('Cant create destination directory %s' % CONFIG['templates']['destination_directory'])
                raise BaseException('Can\'t create directory')

        # Loader for templates
        template_basedir = os.path.dirname(CONFIG['templates']['template_file'])
        log.debug('Template basedir is %s' % template_basedir)
        if not os.path.exists(template_basedir):
            log.error('Directory where templates should be in does not exist, this is not good')
            raise BaseException('Template directory not available, cannot continue')

        # Initialise jinja environment for our files
        jinja2_env = Environment(loader=FileSystemLoader(template_basedir), trim_blocks=False)
        # Add extra methods to template
        jinja2_env.globals.update(dns_resolve=dns_resolve)
        jinja2_env.globals.update(json_decode=json_decode)

        # Start iterator
        while True:
            log.debug('Starting iteration execution')
            # Initialise our internal memory dictionary for reloads
            if os.path.isfile(CONFIG['templates']['reload_struct_file']):
                reload_struct = json_load_file(CONFIG['templates']['reload_struct_file'])
            else:
                # Initialise empty reload_struct_file
                reload_struct = {}
                reload_struct['reload_dictionary'] = {}
                reload_struct['spurious_process'] = {}
                reload_struct['last_nginx_reload'] = 0
                reload_struct['pending_reload'] = False
            # Internal boolean to keep track of reload options, not carried from reload_struct
            do_reload = False
            # Evaluate template so we can use it to render files
            log.debug('Loading template %s' % os.path.basename(CONFIG['templates']['template_file']))
            try:
                template = jinja2_env.get_template(os.path.basename(CONFIG['templates']['template_file']))
            except Exception:
                log.error('Template has errors, please correct them:')
                continue
            ## Generate names and compare against target directory
            filename_list = []
            # Get query with list of things to go through
            if CONFIG['templates']['data_source'].lower() == 'em':
                log.debug('Querying EM server %s with user %s' % (CONFIG['em']['server'], CONFIG['em']['user']))
                em_session = EMApi(server=CONFIG['em']['server'], user=CONFIG['em']['user'], password=CONFIG['em']['password'])
                try:
                    results = em_session.get_upstreams_config()
                except Exception as error:
                    log.debug('We got an error back: %s' % error.message)
                    continue
            if CONFIG['templates']['data_source'].lower() == 'dynamodb':
                log.debug('Querying DynamoDB table %s with key %s comparison operator %s and filter %s' % (CONFIG['dynamodb']['table'], CONFIG['dynamodb']['key'], CONFIG['dynamodb']['comparison_operator'], CONFIG['dynamodb']['filter_expression']))
                results = dynamo_scan(CONFIG['dynamodb']['table'],
                                      CONFIG['dynamodb']['key'],
                                      CONFIG['dynamodb']['filter_expression'],
                                      CONFIG['dynamodb']['comparison_operator'])
            if results is not None:
                # Recover results and iterate
                log.debug('Processing Data results')
                for result in results:
                    # We dont need to trigger a reload yet
                    need_reload = False
                    if CONFIG['templates']['data_field'] == "all":
                        result_data = result
                    else:
                        result_data = result[CONFIG['templates']['data_field']]
                    if CONFIG['templates']['json_decode']:
                        result_data = json_decode(result_data)
                        if result_data is None:
                            log.error('Can\'t decode data for file, skipping')
                            continue
                    # Filter operator
                    if CONFIG['templates']['data_operator'].lower() not in ['startswith', 'contains', 'endswith', 'all']:
                        log.error('Invalid data_operator in config, skipping')
                        continue
                    if CONFIG['templates']['data_operator'].lower() == 'all':
                        log.debug('Magic data operator all used, continuing')
                    elif CONFIG['templates']['data_operator'].lower() == 'startswith':
                        log.debug('Data operator startswith used')
                        if not result_data['UpstreamName'].startswith(tuple(CONFIG['templates']['data_filter'])):
                            log.debug('Upstream %s name doesnt match %s with tuple %s' % (result_data['UpstreamName'], CONFIG['templates']['data_operator'], CONFIG['templates']['data_filter']))
                            continue
                    elif CONFIG['templates']['data_operator'].lower() == 'contains':
                        log.debug('Data operator contains used')
                        if not result_data['UpstreamName'].contains(tuple(CONFIG['templates']['data_filter'])):
                            log.debug('Upstream %s name doesnt match %s with tuple %s' % (result_data['UpstreamName'], CONFIG['templates']['data_operator'], CONFIG['templates']['data_filter']))
                            continue
                    elif CONFIG['templates']['data_operator'].lower() == 'endswith':
                        log.debug('Data operator endswith used')
                        if not result_data['UpstreamName'].endswith(tuple(CONFIG['templates']['data_filter'])):
                            log.debug('Upstream %s name doesnt match %s with tuple %s' % (result_data['UpstreamName'], CONFIG['templates']['data_operator'], CONFIG['templates']['data_filter']))
                            continue
                    else:
                        log.debug('Not understanding data_operator, skipping')
                        continue
                    # Render computed template
                    computed_template = template.render(**{'data': result_data})
                    # Build destination filename
                    filename_only_template = Environment().from_string(CONFIG['templates']['filename_pattern']).render(**{'data': result_data})
                    filename_list.append(filename_only_template)
                    # Compare rendered template to current file if it exists, decide to write or not
                    template_filename = "%s/%s" % (CONFIG['templates']['destination_directory'], filename_only_template)
                    write_file = True
                    if os.path.isfile(template_filename):
                        with open(template_filename, "r") as original_file:
                            orig_file_string = original_file.read()
                            if re.sub('[ \n]', '', orig_file_string) == re.sub('[ \n]', '', computed_template):
                                log.debug('File %s has not changed' % template_filename)
                                write_file = False
                            else:
                                log.info('File %s changed, refreshing' % template_filename)
                                # Gather upstream data here
                                for regenerated_host in result_data['Hosts']:
                                    dns_regenerated_host = dns_resolve(regenerated_host['DnsName'])
                                    if regenerated_host['State'].lower() == 'down':
                                        regenerated_host_offline_status = ' (offline)'
                                    else:
                                        regenerated_host_offline_status = ''
                                    log.info('File %s has host %s with %s upstreams%s' % (template_filename, regenerated_host['DnsName'], len(dns_regenerated_host), regenerated_host_offline_status))
                    if write_file:
                        # they're different, overwrite
                        log.info('Writing file %s' % template_filename)
                        with open(template_filename, "wb") as template_newfile:
                            template_newfile.write(computed_template)
                        need_reload = True

                    ## Reload process
                    if need_reload and CONFIG['templates']['reload_command']:
                        log.info('Reload trigger function start')
                        timestamp_now = time.time()
                        # Check that we dont have spurious reloads
                        if template_filename in reload_struct['reload_dictionary']:
                            # We have already a key, read timestamps
                            for timestamp in reload_struct['reload_dictionary'][template_filename]:
                                do_reload = True
                                time_last_single_reload = int(timestamp_now - timestamp)
                                if time_last_single_reload < int(CONFIG['templates']['reload_time_limit']):
                                    # Check if our object is in the reload_struct['spurious_process'] list
                                    if template_filename in reload_struct['spurious_process']:
                                        # Define the cool off time horizon to see if we're okay again or not
                                        cool_off_time_horizon = timestamp_now - int(CONFIG['templates']['reload_single_upstream_limit_cool_off_time'])
                                        # Check if we have any timestamps in our list, otherwise assume a sane default of cool_off_time_horizon + 1
                                        if len(reload_struct['spurious_process'][template_filename]) > 0:
                                            spurious_process_last_reload = reload_struct['spurious_process'][template_filename][-1]
                                        else:
                                            spurious_process_last_reload = cool_off_time_horizon + 1
                                        if spurious_process_last_reload < cool_off_time_horizon:
                                            # Its not a spurious process anymore, remove from list
                                            del reload_struct['spurious_process'][template_filename]
                                            log.debug('Removing resource %s from spurious list' % template_filename)
                                        else:
                                            # We got a spurious process that requested reload before allowed time
                                            log.info('We have reloaded resource %s %s seconds ago, it is requesting too many reloads' % (template_filename, time_last_single_reload))
                                            # Gather previous iterations and make a decision
                                            spurious_level = 0
                                            # Check each one of the timestamps and see if they match our catching time horizon
                                            catching_time_horizon = timestamp_now - int(CONFIG['templates']['reload_single_upstream_limit_trigger_catching_time'])
                                            for reload_time in reload_struct['spurious_process'][template_filename]:
                                                if reload_time > catching_time_horizon:
                                                    spurious_level += 1
                                            if spurious_level >= int(CONFIG['templates']['reload_single_upstream_limit_trigger']):
                                                # This is a spurious process and confirmed as such
                                                last_single_reload_diff = timestamp_now - time_last_single_reload
                                                if last_single_reload_diff > int(CONFIG['templates']['reload_single_upstream_limit_time']):
                                                    log.info('Will reload nginx on behalf of spurious process %s' % template_filename)
                                                    do_reload = True
                                                else:
                                                    log.info('Blocking process %s from reloading nginx' % template_filename)
                                                    do_reload = False
                                    else:
                                        # New entry, add and pass
                                        reload_struct['spurious_process'][template_filename] = []
                                        do_reload = True
                        else:
                            # We dont have a key, add our first key and proceed
                            reload_struct['reload_dictionary'][template_filename] = []
                            do_reload = True

                        if do_reload:
                            if reload_struct['last_nginx_reload'] == 0:
                                log.info('We have no record of nginx reloads for this thread yet, reloading')
                                reload_struct['reload_dictionary'][template_filename].append(timestamp_now)
                            else:
                                time_last_reload = int(timestamp_now - reload_struct['last_nginx_reload'])
                                if time_last_reload < int(CONFIG['templates']['reload_time_limit']):
                                    log.info('We have reloaded nginx %s seconds ago, will not reload now' % time_last_reload)
                                    do_reload = False
                                    reload_struct['pending_reload'] = True
                                else:
                                    log.info('Our reload threshold is above %s (%s) seconds, reloading' % (int(CONFIG['templates']['reload_time_limit']), time_last_reload))
                                    reload_struct['reload_dictionary'][template_filename].append(timestamp_now)

                if do_reload or reload_struct['pending_reload']:
                    reload_struct['last_nginx_reload'] = timestamp_now
                    reload_struct['pending_reload'] = False
                    program_run_returncode, program_run_output = reload_program(CONFIG['templates']['reload_command'])
                    if program_run_returncode != 0 and CONFIG['templates']['reload_sns_alert_endpoints']:
                        sns_message = "Program failed to reload: %s\n%s" % (CONFIG['templates']['reload_command'], program_run_output)
                        for sns_endpoint in CONFIG['templates']['reload_sns_alert_endpoints']:
                            sns_connection = snsconn()
                            sns_connection.publish(topic=sns_endpoint, message=sns_message)

                    # SNS Alert when things are changed
                    if CONFIG['templates']['reload_sns_notification_endpoints']:
                        sns_message = "Program reloaded: %s" % CONFIG['templates']['reload_command']
                        for sns_endpoint in CONFIG['templates']['reload_sns_notification_endpoints']:
                            sns_connection = snsconn()
                            sns_connection.publish(topic=sns_endpoint, message=sns_message)

                ## Write new reload_struct
                json_reload_struct = json_encode(reload_struct)
                with open(CONFIG['templates']['reload_struct_file'], 'w') as reload_struct_file:
                    fcntl.flock(reload_struct_file, fcntl.LOCK_EX)
                    reload_struct_file.write(json_reload_struct)

                ## Compare filename_list against our local directory and remove unnecessary files
                log.debug('Checking if we need to remove any files')
                local_directory = os.walk(CONFIG['templates']['destination_directory'])
                for _, _, local_files in local_directory:
                    for local_file in local_files:
                        if local_file not in filename_list:
                            full_local_filename = "%s/%s" % (CONFIG['templates']['destination_directory'], local_file)
                            log.debug('Checking file %s' % full_local_filename)
                            # Remove this file as it shoudldn't be here
                            log.info('Removing file %s' % full_local_filename)
                            try:
                                os.remove(full_local_filename)
                            except OSError:
                                log.debug('Can\'t delete file %s, continuing' % full_local_filename)

            ## Sleep our alloted time and start again
            log.debug('Sleeping for %s seconds' % CONFIG['templates']['refresh_rate'])
            time.sleep(int(CONFIG['templates']['refresh_rate']))
    except Exception as error:
        log.error('Caught exception in worker thread: %s' % error.message)
        raise error

def asg_refresher_thread_runner():
    """ ASG Table refresher """
    log = LogWrapper()
    # Enable debug if needed
    if CONFIG['asg']['debug']:
        logging.getLogger().setLevel(logging.DEBUG)
    try:
        if CONFIG['asg']['use_asg_dynamodb']:
            while True:
                log.debug('Starting iteration execution')
                log.debug('Scanning table InfraAsgIPs')
                results = dynamo_scan('InfraAsgIPs')
                if results is not None:
                    log.debug('Converting results to JSON')
                    json_results = json_encode(results)
                    log.debug('Writing file %s' % CONFIG['asg']['asg_file'])
                    with open(CONFIG['asg']['asg_file'], 'w') as asg_file:
                        fcntl.flock(asg_file, fcntl.LOCK_EX)
                        asg_file.write(json_results)
                else:
                    log.debug('No results found in table InfraAsgIPs')
                log.debug('Sleeping for %s seconds' % CONFIG['asg']['refresh_rate'])
                time.sleep(int(CONFIG['asg']['refresh_rate']))
    except Exception as error:
        log.error('Caught exception in worker thread: %s' % error.message)
        raise error

def consul_refresher_thread_runner():
    """ Consul refresher """
    log = LogWrapper()
    # Disable requests logging
    logging.getLogger("requests").setLevel(logging.WARNING)
    logging.getLogger("urllib3").setLevel(logging.WARNING)
    # Enable debug if needed
    if CONFIG['consul']['debug']:
        logging.getLogger().setLevel(logging.DEBUG)
    try:
        if CONFIG['consul']['use_consul']:
            while True:
                log.debug('Starting iteration execution')
                log.debug('Received mappings: %s' % CONFIG['consul']['consul_mappings'])
                for mapping in CONFIG['consul']['consul_mappings']:
                    log.debug('Processing mapping: %s' % mapping)
                    consul_results = []
                    if len(mapping.split('|')) == 4:
                        consul_dc = mapping.split('|')[0]
                        consul_envs = mapping.split('|')[1].split(';')
                        consul_servers = mapping.split('|')[2].split(';')
                        consul_token = mapping.split('|')[3]
                    elif len(mapping.split('|')) == 3:
                        consul_dc = mapping.split('|')[0]
                        consul_envs = mapping.split('|')[1].split(';')
                        consul_servers = mapping.split('|')[2].split(';')
                        consul_token = None
                    else:
                        log.debug('Mapping is not valid')
                        # Mapping is not valid, skipping
                        continue
                    log.debug('Consul mappings for env %s with servers %s and token %s' % (consul_envs, consul_servers, consul_token))
                    consul_need_info = True
                    for consul_server in consul_servers:
                        if consul_need_info:
                            log.debug('Connecting to server %s for dc %s' % (consul_server, consul_dc))
                            log.debug('Using token %s on server %s' % (consul_token, consul_server))
                            params = {}
                            consul_url_base = "http://%s:8500" % consul_server
                            if consul_token:
                                params.update({'token': consul_token})
                            # Get list of services
                            consul_url_services = "%s/v1/catalog/services" % consul_url_base
                            try:
                                consul_services_raw = requests.get(consul_url_services, params=params)
                            except Exception as error:
                                log.debug('Cant connect to Consul: %s' % error.message)
                                continue
                            try:
                                consul_services = consul_services_raw.json()
                            except Exception as error:
                                log.debug('JSON Decode error: %s' % error.message)
                                continue
                            # At this point we dont care if we get info back or not, we're okay
                            consul_need_info = False
                            log.debug('Reported consul services: %s' % consul_services)
                            # Iterate through services and get IPs
                            for consul_service in consul_services:
                                # Check that it matches our environments so we only match services that are of interest to us
                                log.debug('Checking matches for consul service %s' % consul_service)
                                for consul_env in consul_envs:
                                    if consul_service.startswith(consul_env):
                                        log.debug('MATCH FOUND for consul service %s' % consul_service)
                                        consul_ip_array = []
                                        consul_health_service_url = "%s/v1/health/service/%s" % (consul_url_base, consul_service)
                                        try:
                                            consul_health_services_raw = requests.get(consul_health_service_url, params=params)
                                        except Exception:
                                            continue
                                        try:
                                            consul_health_services = consul_health_services_raw.json()
                                        except Exception:
                                            continue
                                        for consul_health_service in consul_health_services:
                                            consul_service_health = True
                                            consul_service_node_ip = None
                                            if 'Address' in consul_health_service['Service']:
                                                # We got address, check that we have the right stuff
                                                if consul_health_service['Service']['Address'] is not '127.0.0.1' or consul_health_service['Service']['Address'] is not '':
                                                    consul_service_node_ip = consul_health_service['Service']['Address']
                                                else:
                                                    log.info('Service %s with incorrect IP definition %s in Service' % (consul_service, consul_health_service['Service']['Address']))
                                            if 'Address' in consul_health_service['Node'] and consul_service_node_ip is None:
                                                if consul_health_service['Node']['Address'] is not '127.0.0.1' or consul_health_service['Node']['Address'] is not '':
                                                    consul_service_node_ip = consul_health_service['Node']['Address']
                                                else:
                                                    log.info('Service %s with incorrect IP definition %s in Node' % (consul_service, consul_health_service['Node']['Address']))
                                            if consul_service_node_ip is None:
                                                log.info('Service %s does not have a correct IP definition, skipping' % consul_service)
                                                continue
                                            for health_check in consul_health_service['Checks']:
                                                if health_check['Status'] == 'critical':
                                                    consul_service_health = False
                                                    log.debug('Service check critical on node %s' % consul_service_node_ip)
                                            if consul_service_health:
                                                consul_ip_array.append(consul_service_node_ip)
                                        consul_results.append({'name': consul_service, 'ip_array': consul_ip_array})
                                        log.debug('Publishing service %s with IPs %s to array %s' % (consul_service, consul_ip_array, consul_results))
                    # Write consul results file
                    if len(consul_results) > 0:
                        json_results = json_encode(consul_results)
                        consul_filename = "%s/consul_%s.json" % (CONFIG['consul']['file_dir'], consul_dc)
                        log.debug('Writing file %s' % consul_filename)
                        log.debug('%s - Got %s results' % (consul_dc, len(consul_results)))
                        with open(consul_filename, 'w') as consul_file:
                            fcntl.flock(consul_file, fcntl.LOCK_EX)
                            consul_file.write(json_results)
                    else:
                        log.debug('Nothing to write for DC %s, either Consul is empty or there is a problem' % consul_dc)
                log.debug('Sleeping for %s seconds' % CONFIG['consul']['refresh_rate'])
                time.sleep(int(CONFIG['consul']['refresh_rate']))
    except Exception as error:
        log.error('Caught exception in worker thread: %s' % error.message)
        raise error

def main():
    """ Main program """
    # Populate env vars (if available)
    try:
        aws_access = os.environ['AWS_ACCESS_KEY_ID']
        aws_key = os.environ['AWS_SECRET_ACCESS_KEY']
    except KeyError:
        aws_access = aws_key = False

    # Parse args
    parser = argparse.ArgumentParser()
    parser.add_argument('-p', '--profile', '--aws_profile', help="Recover credentials from IAM profile", action='store_true', default=False)
    parser.add_argument('-r', '--region', help="AWS Region", default="eu-west-1")
    parser.add_argument('-c', '--config', help="Upstreamr config", default="/etc/upstreamr/upstreamr.conf")
    parser.add_argument('-d', '--debug', help="Debug mode", action='store_true', default=False)
    parser.add_argument('-v', '--verbose', help="Show logs through console", action='store_true', default=False)
    parser.add_argument('-l', '--log', help="Logfile location", default="/var/log/upstreamr.log")
    parser.add_argument('--aws_access_key', help="AWS Access Key", default=aws_access)
    parser.add_argument('--aws_secret_key', help="AWS Secret Key", default=aws_key)
    global ARGS
    ARGS = parser.parse_args()

    # Enable logging facilities
    if ARGS.debug:
        loglevel = logging.DEBUG
    else:
        loglevel = logging.INFO
    # Initialise logger
    console_loglevel = logfile_loglevel = logging.getLevelName(loglevel)
    logging.basicConfig(level=loglevel, format='%(asctime)s %(name)s %(levelname)s %(message)s')
    logger = logging.getLogger()
    logger.handlers = []
    logger.setLevel(console_loglevel)
    # create file handler
    logfile_handler = logging.FileHandler(ARGS.log, mode='a')
    logfile_handler.setLevel(logfile_loglevel)
    # create console handler with a higher log level
    console_handler = logging.StreamHandler()
    console_handler.setLevel(console_loglevel)
    # create formatter and add it to the handlers
    formatter = logging.Formatter(
        '%(asctime)s %(name)s %(levelname)s %(message)s', '%b %d %H:%M:%S')
    logfile_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    # add the handlers to the logger
    if ARGS.verbose:
        logger.addHandler(console_handler)
    logger.addHandler(logfile_handler)

    log = LogWrapper()
    log.info('Starting Upstreamr')
    log.debug('Created logger facilities')

    unparsed_config = configparser.SafeConfigParser(allow_no_value=True)
    try:
        with open(ARGS.config):
            unparsed_config.read(ARGS.config)
    except Exception:
        log.critical('Can\'t read config file: %s' % ARGS.config)
        sys.exit(1)

    ### Sanitise config values and adopt default values
    ## Check that sections exist
    log.debug('Sanitising input config')
    for section in ['asg', 'dynamodb', 'templates']:
        if not unparsed_config.has_section(section):
            unparsed_config.add_section(section)

    ## Check that critical options are in place, otherwise throw an error
    # Dynamodb section
    for dynamodb_option in ['table', 'key', 'filter_expression']:
        if not unparsed_config.has_option('dynamodb', dynamodb_option):
            raise configparser.NoOptionError(dynamodb_option, 'dynamodb')
    # EM section
    for em_option in ['server', 'user', 'password']:
        if not unparsed_config.has_option('em', em_option):
            raise configparser.NoOptionError(em_option, 'em')
    # ASG section
    if unparsed_config.get('asg', 'use_asg_dynamodb') is True:
        for dynamodb_asg_option in ['asg_table', 'asg_key', 'asg_value']:
            if not unparsed_config.has_option('asg', dynamodb_asg_option):
                raise configparser.NoOptionError(dynamodb_asg_option, 'asg')
    # Templates section
    for templates_option in ['destination_directory', 'template_file', 'data_field', 'filename_pattern']:
        if not unparsed_config.has_option('templates', templates_option):
            raise configparser.NoOptionError(templates_option, 'templates')

    ## Fill not critical values if they're empty or unexisting
    # Templates
    if not (unparsed_config.has_option('templates', 'refresh_rate') or unparsed_config.get('templates', 'refresh_rate') == ''):
        unparsed_config.set('templates', 'refresh_rate', 30)
    if not (unparsed_config.has_option('templates', 'debug') or unparsed_config.get('templates', 'debug') == ''):
        unparsed_config.set('templates', 'debug', False)
    if not (unparsed_config.has_option('dynamodb', 'comparison_operator') or unparsed_config.get('dynamodb', 'comparison_operator') == ''):
        unparsed_config.set('dynamodb', 'comparison_operator', 'contains')
    if not (unparsed_config.has_option('templates', 'json_decode') or unparsed_config.get('templates', 'json_decode') == ''):
        unparsed_config.set('templates', 'json_decode', False)
    if not (unparsed_config.has_option('templates', 'reload_sns_alert_endpoints') or unparsed_config.get('templates', 'reload_sns_alert_endpoints') == ''):
        unparsed_config.set('templates', 'reload_sns_alert_endpoints', None)
    if not (unparsed_config.has_option('templates', 'reload_sns_notification_endpoints') or unparsed_config.get('templates', 'reload_sns_notification_endpoints') == ''):
        unparsed_config.set('templates', 'reload_sns_notification_endpoints', None)
    if not (unparsed_config.has_option('templates', 'reload_command') or unparsed_config.get('templates', 'reload_command') == ''):
        unparsed_config.set('templates', 'reload_command', None)
    if not (unparsed_config.has_option('templates', 'data_source') or unparsed_config.get('templates', 'data_source') == ''):
        unparsed_config.set('templates', 'data_source', 'em')
    if not (unparsed_config.has_option('templates', 'reload_struct_file') or unparsed_config.get('templates', 'reload_struct_file') == ''):
        unparsed_config.set('templates', 'reload_struct_file', '/tmp/upstreamr_reload_struct.json')
    if not (unparsed_config.has_option('templates', 'reload_time_limit') or unparsed_config.get('templates', 'reload_time_limit') == ''):
        unparsed_config.set('templates', 'reload_time_limit', 30)
    if not (unparsed_config.has_option('templates', 'reload_single_upstream_limit_trigger') or unparsed_config.get('templates', 'reload_single_upstream_limit_trigger') == ''):
        unparsed_config.set('templates', 'reload_single_upstream_limit_trigger', 3)
    if not (unparsed_config.has_option('templates', 'reload_single_upstream_limit_trigger_catching_time') or unparsed_config.get('templates', 'reload_single_upstream_limit_trigger_catching_time') == ''):
        unparsed_config.set('templates', 'reload_single_upstream_limit_trigger_catching_time', 300)
    if not (unparsed_config.has_option('templates', 'reload_single_upstream_limit_time') or unparsed_config.get('templates', 'reload_single_upstream_limit_time') == ''):
        unparsed_config.set('templates', 'reload_single_upstream_limit_time', 120)
    if not (unparsed_config.has_option('templates', 'reload_single_upstream_limit_cool_off_time') or unparsed_config.get('templates', 'reload_single_upstream_limit_cool_off_time') == ''):
        unparsed_config.set('templates', 'reload_single_upstream_limit_cool_off_time', 600)

    # ASG
    if not (unparsed_config.has_option('asg', 'use_asg_dynamodb') or unparsed_config.get('asg', 'use_asg_dynamodb') == ''):
        unparsed_config.set('asg', 'use_asg_dynamodb', False)
    if not (unparsed_config.has_option('asg', 'asg_file') or unparsed_config.get('asg', 'asg_file') == ''):
        unparsed_config.set('asg', 'asg_file', '/tmp/asg_upstreamr.json')
    if not (unparsed_config.has_option('asg', 'refresh_rate') or unparsed_config.get('asg', 'refresh_rate') == ''):
        unparsed_config.set('asg', 'refresh_rate', 20)
    if not (unparsed_config.has_option('asg', 'debug') or unparsed_config.get('asg', 'debug') == ''):
        unparsed_config.set('asg', 'debug', False)
    # Consul
    if not (unparsed_config.has_option('consul', 'use_consul') or unparsed_config.get('consul', 'use_consul') == ''):
        unparsed_config.set('consul', 'use_consul', False)
    if not (unparsed_config.has_option('consul', 'file_dir') or unparsed_config.get('consul', 'file_dir') == ''):
        unparsed_config.set('consul', 'file_dir', '/tmp')
    if not (unparsed_config.has_option('consul', 'refresh_rate') or unparsed_config.get('consul', 'refresh_rate') == ''):
        unparsed_config.set('consul', 'refresh_rate', 5)
    if not (unparsed_config.has_option('consul', 'debug') or unparsed_config.get('consul', 'debug') == ''):
        unparsed_config.set('consul', 'debug', False)
    # EM
    if not (unparsed_config.has_option('em', 'server') or unparsed_config.get('em', 'server') == ''):
        unparsed_config.set('em', 'server', 'localhost')
    if not (unparsed_config.has_option('em', 'user') or unparsed_config.get('em', 'user') == ''):
        unparsed_config.set('em', 'user', None)
    if not (unparsed_config.has_option('em', 'password') or unparsed_config.get('em', 'password') == ''):
        unparsed_config.set('em', 'password', None)

    # Write finished config
    global CONFIG
    log.debug('Created parsed config global')
    CONFIG = unparsed_config._sections

    # Bool converter
    CONFIG['asg']['use_asg_dynamodb'] = to_bool(CONFIG['asg']['use_asg_dynamodb'])
    CONFIG['consul']['use_consul'] = to_bool(CONFIG['consul']['use_consul'])
    CONFIG['templates']['json_decode'] = to_bool(CONFIG['templates']['json_decode'])
    CONFIG['consul']['debug'] = to_bool(CONFIG['consul']['debug'])
    CONFIG['templates']['debug'] = to_bool(CONFIG['templates']['debug'])
    CONFIG['asg']['debug'] = to_bool(CONFIG['asg']['debug'])

    # Array converter
    CONFIG['dynamodb']['filter_expression'] = to_list(CONFIG['dynamodb']['filter_expression'])
    CONFIG['consul']['consul_mappings'] = to_list(CONFIG['consul']['consul_mappings'])
    CONFIG['templates']['data_filter'] = to_list(CONFIG['templates']['data_filter'])
    CONFIG['templates']['reload_sns_notification_endpoints'] = to_list(CONFIG['templates']['reload_sns_notification_endpoints'])
    CONFIG['templates']['reload_sns_alert_endpoints'] = to_list(CONFIG['templates']['reload_sns_alert_endpoints'])

    # Launch process threads
    template_runner = multiprocessing.Process(name='template_runner', target=template_thread_runner)
    asg_refresher_runner = multiprocessing.Process(name='asg_refresher_runner', target=asg_refresher_thread_runner)
    consul_refresher_runner = multiprocessing.Process(name='consul_refresher_runner', target=consul_refresher_thread_runner)

    running_processes = []
    # Launch processes
    if CONFIG['asg']['use_asg_dynamodb']:
        log.info('Starting ASG Table Refresher thread')
        asg_refresher_runner.daemon = True
        asg_refresher_runner.start()
        running_processes.append(asg_refresher_runner)
    else:
        log.info('No asg runner')
    if CONFIG['consul']['use_consul']:
        log.info('Starting Consul Refresher thread')
        consul_refresher_runner.daemon = True
        consul_refresher_runner.start()
        running_processes.append(consul_refresher_runner)
    else:
        log.info('No consul runner')
    # I want to sleep a bit before I start the template engine so ASG can populate
    log.info('Starting template engine')
    template_runner.daemon = True
    template_runner.start()
    running_processes.append(template_runner)

    # Check that we keep things alive, otherwise kick them switfly
    while True:
        time.sleep(5)
        for process in running_processes:
            log.debug('ProcessWatchDog - Checking process %s' % process.name)
            restart_process = False
            log.debug('Process %s is %s and exitcode %s' % (process.name, process.is_alive(), process.exitcode))
            if process.exitcode is None and not process.is_alive():
                log.info('ProcessWatchDog - Process %s didn\'t finish and is not running, restarting' % process.name)
                restart_process = True
            elif process.exitcode is not None and process.exitcode is not 0:
                log.info('ProcessWatchDog - Process %s ended with an error code %s, restarting' % (process.name, process.exitcode))
                restart_process = True
            if restart_process:
                log.info('Terminating process: %s' % process.name)
                process.terminate()
                running_processes.remove(process)
                if process.name == 'template_runner':
                    log.info('Starting new process template_runner')
                    template_runner = multiprocessing.Process(name='template_runner', target=template_thread_runner)
                    template_runner.daemon = True
                    template_runner.start()
                    running_processes.append(template_runner)
                elif process.name == 'asg_refresher_runner':
                    log.info('Starting new process asg_refresher_runner')
                    asg_refresher_runner = multiprocessing.Process(name='asg_refresher_runner', target=asg_refresher_thread_runner)
                    asg_refresher_runner.daemon = True
                    asg_refresher_runner.start()
                    running_processes.append(asg_refresher_runner)
                elif process.name == 'consul_refresher_runner':
                    log.info('Starting new process consul_refresher_runner')
                    consul_refresher_runner = multiprocessing.Process(name='consul_refresher_runner', target=consul_refresher_thread_runner)
                    consul_refresher_runner.daemon = True
                    consul_refresher_runner.start()
                    running_processes.append(consul_refresher_runner)

if __name__ == "__main__":
    main()
